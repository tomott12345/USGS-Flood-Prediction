{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "029c8dee-8638-4d10-988e-1f0730c6c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from https://nwis.waterservices.usgs.gov/nwis/iv/?sites=01473730&parameterCd=00065&startDT=2022-11-11T07:42:38.000-05:00&endDT=2024-11-10T07:42:38.000-05:00&siteStatus=all&format=rdb with skiprows=24\n",
      "Data loaded successfully from https://waterservices.usgs.gov/nwis/iv/?sites=01473730&parameterCd=00060&startDT=2022-11-11T07:42:38.000-05:00&endDT=2024-11-10T07:42:38.000-05:00&siteStatus=all&format=rdb with skiprows=24\n",
      "Data merged and index reset successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>328674_00065</th>\n",
       "      <th>328771_00060</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-16 12:45:00</td>\n",
       "      <td>7.18</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-16 13:15:00</td>\n",
       "      <td>7.18</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-16 13:30:00</td>\n",
       "      <td>7.19</td>\n",
       "      <td>1410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-16 13:45:00</td>\n",
       "      <td>7.19</td>\n",
       "      <td>1410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-16 14:00:00</td>\n",
       "      <td>7.16</td>\n",
       "      <td>1370.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  328674_00065  328771_00060\n",
       "0 2023-05-16 12:45:00          7.18        1400.0\n",
       "1 2023-05-16 13:15:00          7.18        1400.0\n",
       "2 2023-05-16 13:30:00          7.19        1410.0\n",
       "3 2023-05-16 13:45:00          7.19        1410.0\n",
       "4 2023-05-16 14:00:00          7.16        1370.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "end_date = datetime.now().strftime('%Y-%m-%dT%H:%M:%S.000-05:00')\n",
    "begin_date = (datetime.now() - timedelta(days=2*365)).strftime('%Y-%m-%dT%H:%M:%S.000-05:00')\n",
    "\n",
    "# USGS streamgage site\n",
    "site_code = '01473730'  # Schuylkill River at Conshohocken, PA\n",
    "\n",
    "# URLs for gage height and flow data\n",
    "gage_url = f'https://nwis.waterservices.usgs.gov/nwis/iv/?sites={site_code}&parameterCd=00065&startDT={begin_date}&endDT={end_date}&siteStatus=all&format=rdb'\n",
    "flow_url = f'https://waterservices.usgs.gov/nwis/iv/?sites={site_code}&parameterCd=00060&startDT={begin_date}&endDT={end_date}&siteStatus=all&format=rdb'\n",
    "\n",
    "# Function to load and merge data with dynamic skiprows and column filtering\n",
    "def load_data(gage_url, flow_url, skip_options=range(24, 30)):\n",
    "    def fetch_data(url, column_suffix):\n",
    "        for skip in skip_options:\n",
    "            try:\n",
    "                # Attempt to read data with specified skiprows\n",
    "                data = pd.read_csv(url, sep='\\t', skiprows=skip, comment='#')\n",
    "                \n",
    "                # Drop fully empty columns, which are usually metadata\n",
    "                data = data.dropna(how=\"all\", axis=1)\n",
    "                \n",
    "                # Select datetime and the specific measurement column based on suffix\n",
    "                measurement_col = [col for col in data.columns if col.endswith(column_suffix)]\n",
    "                if not measurement_col:\n",
    "                    raise ValueError(f\"No column ending with {column_suffix} found.\")\n",
    "                \n",
    "                # Convert measurement column to numeric, handling non-numeric entries as NaN\n",
    "                data[measurement_col[0]] = pd.to_numeric(data[measurement_col[0]], errors='coerce')\n",
    "                \n",
    "                # Drop rows with missing datetime or measurement values\n",
    "                data = data.dropna(subset=['datetime', measurement_col[0]])\n",
    "                \n",
    "                # Set datetime as index\n",
    "                data = data[['datetime', measurement_col[0]]].set_index('datetime')\n",
    "                data.index = pd.to_datetime(data.index, errors='coerce')\n",
    "                \n",
    "                # Ensure datetime conversion succeeded\n",
    "                if data.index.isnull().any():\n",
    "                    raise ValueError(\"Datetime conversion failed.\")\n",
    "                \n",
    "                print(f\"Data loaded successfully from {url} with skiprows={skip}\")\n",
    "                return data, measurement_col[0]\n",
    "            except (ValueError, KeyError, IndexError, pd.errors.ParserError) as e:\n",
    "                print(f\"Failed with skiprows={skip} for {url}: {e}\")\n",
    "        \n",
    "        raise ValueError(f\"Failed to load data from {url} with any specified skiprows option.\")\n",
    "    \n",
    "    # Load gage and flow data\n",
    "    gage_data, gage_col_name = fetch_data(gage_url, '_00065')\n",
    "    flow_data, flow_col_name = fetch_data(flow_url, '_00060')\n",
    "\n",
    "    # Merge the datasets on the datetime index\n",
    "    df = pd.merge(gage_data, flow_data, how='inner', left_index=True, right_index=True)\n",
    "    df.columns = [gage_col_name, flow_col_name]  # Rename columns with detected names\n",
    "\n",
    "    # Reset the index to make datetime a regular column\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    print(\"Data merged and index reset successfully.\")\n",
    "    return df\n",
    "\n",
    "# Load and merge data from both gage and flow sources\n",
    "data = load_data(gage_url, flow_url)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9afdc-0717-44be-9e72-6be47ea1c1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984bec5-5e64-4349-9b9b-7f20b37b2cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
